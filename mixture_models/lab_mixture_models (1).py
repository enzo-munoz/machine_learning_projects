#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import pandas as pd
from scipy.stats import norm as normal
from scipy.stats import multivariate_normal as mvn

get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")


# In[2]:


from IPython.core.display import Image, display


# In[3]:


get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.image as mpimg
plt.rcParams["figure.figsize"] = [6,6]
plt.rcParams['font.size'] = 12


# ## Degree in Data Science and Engineering, group 96
# ## Machine Learning 2
# ### Fall 2020
# 
# &nbsp;
# &nbsp;
# &nbsp;
# # Lab 6. Mixture Models
# 
# &nbsp;
# &nbsp;
# &nbsp;
# 
# **Jose Manuel de Frutos Porras and David Martínez Rubio**
# 
# **Adapted from a notebook by Ignacio Peis**
# 
# Dept. of Signal Processing and Communications
# 
# &nbsp;
# &nbsp;
# &nbsp;
# 
# 
# 
# 
# 

# # Introduction
# 
# In this lab you will study two mixure models: the Gaussian Mixture Model (GMM) for real-valued data and the Mixture of Bernoullis for data in the range $[0, 1]$. We assume that each dataset $\mathcal{D}$ is composed by $N$ observations $\mathbf{x}_i$ with dimension $D$, and, for the Gaussian part, $\mathbf{x}_i \in \mathbb{R}$, while for the second part, $\mathbf{x}_i \in [0, 1]$.
# 
# ## Mixture models
# 
#  Mixture Models are a special class of probabilistic generative models that can be used for both **density estimation** and **clustering**. Given a set of multidimensional observations, mixture models assume that observed variables are correlated because they arise from a hidden common "cause" or **latent variable** that is discrete (it can only take a finite number of values). Instead of assuming that every sample comes from the same simple distribution, in a mixture model, each observation is expected to "belong" to one among $K$ distributions. There is some probability $\pi_k \in [0, 1]$ that an observation has been generated by component $k$.
# 
# We say that a distribution $p(\mathbf{x})$ is a **mixture of $K$ component** distributions
# $p_{1}(\mathbf{x}), p_{2}(\mathbf{x}),\ldots,p_{K}(\mathbf{x})$ if
# 
# \begin{align}
# p(\mathbf{x})=\sum_{k=1}^{K} \pi_k p_{k}(\mathbf{x}),
# \end{align}
# with $\pi_k$ being the **mixing weights**, $\pi_k>0$, $\sum_k \pi_k =1$. An equivalent way to understad a Mixture Model is to think that every sample $\mathbf{x}$ is generated by first sampling a discrete, hidden R.V. $z\in\{1,\ldots,K\}$ with probabilities $\pi_1,\ldots,\pi_K$, and then a sample $\mathbf{x}$ from $p_k(\mathbf{x})$ is generated. Namely,
# \begin{align}
# p(\mathbf{x})=\int_{z} p(\mathbf{x},z) dz = \sum_{k=1}^{K} \pi_k p_{k}(\mathbf{x}),
# \end{align}
# where
# \begin{align}
# p(\mathbf{x},z) = \prod_{k=1}^{K} \left(\pi_k ~p_{k}(\mathbf{x})\right)^{\mathbb{1} [z==k]}
# \end{align}
# 
# ## The Expectation-Maximization (EM) algorithm
# 
# In this type of model, we denote with $\theta$ all the parameters of the model, including parameters for each component of the distribution (mean and variances for Gaussian, $a$ and $b$ for Bernoulli, etc.) and mixing weights $\pi_k$ (probabilities of each component). For example, for a mixture of Gaussians:
# \begin{equation}
# \theta=\{ \boldsymbol{\pi}, \boldsymbol{\mu}_0, \boldsymbol{\Sigma}_0, \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1, ..., \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K \}
# \end{equation}
# The final goal is to obtain the parameters that best explain the data, and for that purpose, we maximize the log likelihood of the observed data, which is given by:
# \begin{align}
# \ell(\mathbf{\theta})=\sum_{i=1}^{N} \log p(\mathbf{x}^{(i)} | \theta) = \sum_{i=1}^{N} \log \left[\sum_{z^{(i)}} p(\mathbf{x}^{(i)},z^{(i)}|\mathbf{\theta})\right]
# \end{align}
# Unfortunately this is very hard to optimize, since the log cannot be pushed inside the sum. What can we do? The **Expectation Maximization (EM)** algorithm gets around this problem as follows. The **EM** algorithm provides a simple iterative solution that guarantees convergence to a **local mode**. The steps are the following. First, define the **complete data log likelihood** to be the one that includes $z^{(i)}$:
# \begin{align}
# \ell_c(\mathbf{\theta})=\sum_{i=1}^{N} \log  p(\mathbf{x}^{(i)},z^{(i)}|\mathbf{\theta}) = \sum_{i=1}^{N} \sum_{k=1}^{K}  \mathbb{I} [z==k] \left[ \log (\pi_k) + \log p_{k}(\mathbf{x})\right]
# \end{align}
# Now we have the sum out of the logarithm. Nevertheless, the new problem is: how we compute $\mathbb{I} [z==k]$ if $z$ is hidden (i.e., we don't know $z$)? The solution is to compute the expectation over $z$, that consists of taking the whole discrete possibilities of $z$ into account (informally speaking, we model what should happen, would $z$ select component $k$ with our previous estimated probability). We call this the expected complete data log likelihood, which is given by:
# 
# \begin{align}
# \mathcal{Q}(\mathbf{\theta},\mathbf{\theta}_{t-1})&= \mathbb{E}_{z^{(i)}\sim p(z^{(i)}|\mathbf{x}^{(i)},\mathbf{\theta}_{t-1})} \left[ \ell_c(\mathbf{\theta}) \right]=\sum_{i=1}^{N} \mathbb{E}_{z^{(i)}\sim p(z^{(i)}|\mathbf{x}^{(i)},\mathbf{\theta}_{t-1})}\left[\log p(\mathbf{x}^{(i)},z^{(i)})\right]= \\
# &= \sum_{i=1}^{N}\sum_{k=1}^K p(z^{(i)}=k|\mathbf{x}^{(i)},\mathbf{\theta}_{t-1}) \left[\log \pi_k +\log p(\mathbf{x}^{(i)}|\theta_k)\right],
# \end{align}
# 
# You can notice that we are taking expectations over $z$ from its posterior distribution $p(z^{(i)}=k|\mathbf{x}^{(i)},\mathbf{\theta}_{t-1})$, instead of the prior ($p(z^{(i)}=k)=\pi_k$), to include knowledge from data. This posterior of each component, given the data, can be obtained by applying the Bayes rule:
# \begin{align}
# p(z^{(i)}=k|\mathbf{x}^{(i)},\mathbf{\theta}_{t-1}) \triangleq r_{ik} = \frac{\pi_{(k,t-1)} p(\mathbf{x}^{(i)}| \theta_{(k,t-1)} )}{\sum_{q=1}^K \pi_{(q,t-1)} p(\mathbf{x}^{(i)}|\theta_{(q,t-1)})}
# \end{align}
# 
# The values $r_{ik}$ are known as the responsibility that cluster $k$ takes for data point $i$ (given $\mathbf{\theta}_{t-1}$). This is the posterior probability that component $k$ generated $\mathbf{x}_i$.
# 
# As you may appreciate, all these expressions depend on the last value of the parameters of the model: $\theta_{t-1}$. The EM is an iterative algorithm divided into two steps at each iteration, and each iteration depends on the last parameters update. Therefore, we need to initialize the parameters at the start.
# 
# 

# ### E-Step
# In the E-step, we just compute the expected complete data log likelihood $\mathcal{Q}(\mathbf{\theta},\mathbf{\theta}_{t-1})$ to see how likely the data is given the last value of the parameters. As we know $\mathbf{\theta}_{t-1}$, we are going to obtain a function over $\theta$ (the new parameters that we want to obtain).
# 
# 
# ### M-Step
# In the M step, we optimize $\mathcal{Q}$ w.r.t. $\theta$ in order to update the new optimal the parameters:
# \begin{equation}
# \hat{\pi}_k = \frac{\partial \mathcal{Q}(\mathbf{\theta},\mathbf{\theta}_{t-1})}{\partial \pi_k }
# \end{equation}
# For the Gaussian case, for example, we must compute:
# \begin{equation}
# \hat{\boldsymbol{\mu}}_k = \frac{\partial \mathcal{Q}(\mathbf{\theta},\mathbf{\theta}_{t-1})}{\partial \boldsymbol{\mu}_k }  \qquad  \hat{\boldsymbol{\Sigma}}_k = \frac{\partial \mathcal{Q}(\mathbf{\theta},\mathbf{\theta}_{t-1})}{\partial \boldsymbol{\Sigma}_k }
# \end{equation}
# *We have to obtain the same for all the components, i.e. for each $k$.*
# 
# 
# 
# 
# 

# # 1. Gaussian Mixture Model
# 
# A Gaussian Mixture Model (GMM) is composed by $K$ Gaussian components, with the set of parameters $\theta$ composed by component parameters $\{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\}$ for $k=1, ..., K$ and mixing weights $\boldsymbol{\pi}=[\pi_0, \pi_1, ..., \pi_K]$.
# 
# For the GMM, the EM steps are defined as following:
# 
# ### E-Step for GMMs
# The expected complete likelihood and the responsibilities are given by:
# \begin{gather}
# \mathcal{Q}(\mathbf{\theta},\mathbf{\theta}_{t-1}) =
# \sum_{i=1}^{N}\sum_{k=1}^K p(z^{(i)}=k|\mathbf{x}^{(i)},\mathbf{\theta}_{t-1}) \left[\log \pi_k +\log \mathcal{N}(\mathbf{x}^{(i)}|\mathbf{\mu}_k,\mathbf{\Sigma}_k)\right] \\
# p(z^{(i)}=k|\mathbf{x}^{(i)},\mathbf{\theta}_{t-1}) \triangleq r_{ik} = \frac{\pi_{(k,t-1)} \mathcal{N}(\mathbf{x}^{(i)}|\mathbf{\mu}_{(k,t-1)},\mathbf{\Sigma}_{(k,t-1)})}{\sum_{q=1}^K \pi_{(q,t-1)} \mathcal{N}(\mathbf{x}^{(i)}|\mathbf{\mu}_{(q,t-1)},\mathbf{\Sigma}_{(q,t-1)})}
# \end{gather}
# 
# ### M-Step for GMMs
# 
# In the M step, we optimize $\mathcal{Q}$ w.r.t. $\pi_k,\mathbf{\mu}_k,\mathbf{\Sigma}_k$, $k=1,\ldots,K$.
# 
# #### Optimization of $\pi_k$
# 
# It is easy to see that
# \begin{align}
# \pi_k = \frac{\partial \mathcal{Q}(\mathbf{\theta},\mathbf{\theta}_{t-1})}{\partial \pi_k } = \frac{1}{N} \sum_{i=1}^N r_{ik}=\frac{r_k}{N},
# \end{align}
# where $r_k=\sum_{i=1}^N r_{ik}$ is the weighted number of points assigned to cluster $k$.
# 
# #### Optimization of $\mathbf{\mu}_k$, $\mathbf{\Sigma}_k$
# 
# We look at the parts of $\mathcal{Q}$ that depend on $\mathbf{\mu}_k$ and $\mathbf{\Sigma}_k$
# \begin{align}
# f(\mathbf{\mu}_k,\mathbf{\Sigma}_k) = \frac{-1}{2}\sum_{i=1}^N r_{ik} \left[\log |\mathbf{\Sigma}_k| + (\mathbf{x}^{(i)}-\mathbf{\mu}_k)^T\mathbf{\Sigma}^{-1}_k(\mathbf{x}^{(i)}-\mathbf{\mu}_k)\right]
# \end{align}
# 
# and one can show that the new parameter estimates are given by (See Section 9.2.2 of Bishop's Book for details)
# 
# \begin{align}
# \hat{\mathbf{\mu}}_k &=&
# \frac{\partial \mathcal{Q}(\mathbf{\theta},\mathbf{\theta}_{t-1})}{\partial \boldsymbol{\mu}_k } &= \frac{\sum_{i=1}^N r_{ik}\mathbf{x}^{(i)}}{r_k} \\
# \hat{\mathbf{\Sigma}}_k &=&
# \frac{\partial \mathcal{Q}(\mathbf{\theta},\mathbf{\theta}_{t-1})}{\partial \boldsymbol{\Sigma}_k } &= \frac{\sum_{i=1}^N r_{ik}\mathbf{x}^{(i)}(\mathbf{x}^{(i)})^T}{r_k}-\mathbf{\mu}_k\mathbf{\mu}_k^T
# \end{align}
# 
# Thus, the EM for Gaussian Mixture Models is **very easy to implement!** We have to compute $r_{ik}$ given the "old" parameters in the E-Step, and update the parameters using these $r_{ik}$s. Then, repeat until covergence.
# 
# 

# ## 1.1 Scikit-Learn implementation
# 
# Although it remains easy to implement the EM algorithm, we have a [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) for GMMs and the EM algorithm for fitting their parameters. The only thing we have to do (apart from 100% understanding the model!) is creating a GMM object using, for example:
# 
# <code>model = mixture.GaussianMixture(n_components=2, covariance_type='full')</code>
# 
# and call the EM algorithm by using:
# 
# <code>model.fit(X)</code>
# 
# 

# **Check the given link to understand the methods of the GaussianMixture object implemented by sklearn:**
# 
# **1. Which type of covariance gives you an isotropic density for each cluster?**
# - Your answer
# 
# **2. Wrt the predict_proba(X) method. Can you identify which variable of the EM algorithm described above is returning this method?**
# - Your answer
# 
# 

# In[4]:


from sklearn.mixture import GaussianMixture as GMM


# Example of a GMM with K=3 and D=2
# Create a dataset of samples from a GMM with parameters:
mus = np.array([[0,4], [-2,0], [6,-2]])
sigmas = np.array([   [[3, 0], [0, 0.5]], [[1,0],[0,2]], [[3,0],[0,1]] ]) # Diagonal Sigma
pis = np.array([0.3, 0.4, 0.3])

# Sample data from this model (pi*N is the proportion of samples for each component)
N = 250
X_samples= np.concatenate([np.random.multivariate_normal(mu, sigma, int(pi*N))
                    for pi, mu, sigma in zip(pis, mus, sigmas)])

# Plot the original distribution
f, ax= plt.subplots(1,2, figsize=(12, 6))
intervals = 200
x0 = np.linspace(-6, 12, intervals)
x1 = np.linspace(-6, 12, intervals)
X0,X1 = np.meshgrid(x0,x1)
xs = np.vstack([X0.ravel(), X1.ravel()]).T
Y = np.zeros(len(xs))
for pi, mu, sigma in zip(pis, mus, sigmas):
    Y += pi*mvn(mu, sigma).pdf(xs)
Y = Y.reshape([intervals,intervals])
ax[0].contour(X0, X1, Y, 20, cmap='Blues')
ax[0].plot(X_samples[:, 0], X_samples[:, 1], 'x', color='tab:blue', label='Samples')
ax[0].grid()
ax[0].set_xlabel(r'$x_0$')
ax[0].set_ylabel(r'$x_1$')
ax[0].set_title('Sampling from GMM')
ax[0].legend(loc='best')

# Fit a new GMM observing the samples
# verbose=2 for learning purposes, if you prefer set verbose=0
gmm = GMM(n_components=3, covariance_type='diag', verbose=2, verbose_interval=1)
gmm.fit(X_samples)
pred_pis = gmm.weights_
pred_mus = gmm.means_
pred_sigmas = gmm.covariances_

# Plot the fitted GMM
Y = np.zeros(len(xs))
for pi, mu, sigma in zip(pred_pis, pred_mus, pred_sigmas):
    Y += pi*mvn(mu, sigma).pdf(xs)
Y = Y.reshape([intervals,intervals])
ax[1].contour(X0, X1, Y, 20, cmap='Reds')
ax[1].plot(X_samples[:, 0], X_samples[:, 1], 'x', color='tab:red', label='Observations')
ax[1].grid()
ax[1].set_xlabel(r'$x_0$')
ax[1].set_ylabel(r'$x_1$')
ax[1].set_title('Fitting a GMM from observations')
ax[1].legend(loc='best')

print('Original means: ')
print(mus)

print('Fitted means after ' + str(gmm.n_iter_) + ' EM iterations: ')
print(pred_mus)


# In[5]:


# PDF surfplace plot
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X0, X1, Y, cmap='viridis')
ax.set_xlabel(r'$x_0$')
ax.set_ylabel(r'$x_1$')
ax.set_zlabel(r'$p(\mathbf{x})$')
plt.title('Gaussian Mixture Model');


# ## 1.2. Choosing the number of components
# 
# An important question that typically arises when fitting a generative probabilistic model is how to select the right number of components. This problem is known as **model selection**.
# 
# A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the likelihood of the data under the model, **using cross-validation to avoid over-fitting.** Namely, we evaluate the model log-likelihood for a validation set not used for training.
# 
# Alternatively, the optimal Bayesian approach is to pick the model with the largest **marginal likelihood**:
# 
# \begin{align}
# K^* = \arg\max_{K} p(\mathbf{X}|K) = \arg\max_{K} \int_{\mathbf{\theta}} p(\mathbf{X}|\mathbf{\theta},K) p(\mathbf{\theta}|K) d\mathbf{\theta}
# \end{align}
# 
# In general, computing the marginal likelihood is quite difficult. Assuming that the prior distribution $p(\mathbf{\theta}|K)$ is Gaussian and very broad, we can approximate this term by the so-called **Bayesian Information Criterion** (BIC)
# \begin{align}
# -\log p(\mathbf{X}|K) \approx \frac{K\log N}{2}-\log p(\mathbf{X}|\mathbf{\theta}_{MAP},K)
# \end{align}
# 
# where $\mathbf{\theta}_{MAP}$ is the mode of $p(\mathbf{\theta}|\mathbf{X},K)$, which is what we approximate with the EM algorithm (assuming again a very broad prior, that does not weight much if the number of data points is large enough). **See Section 4.4.1. of Bishop's book for more details**. BIC is also very nicely derived in this [link](http://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/BIC.pdf).
# 
# In summary:
# 
# - BIC gives us a way to choose between two different models with different numbers of parameters by selecting the one which yields the **lowest BIC score**.
# 
# - More complex models are almost always likely to fit the data better (and therefore have a lower value of $- \log p(\mathbf{X}|\mathbf{\theta}_{MAP},K)$).
# 
# - BIC gives us a relatively principled way to penalize these extra parameters in the form of the term $\frac{K\log N}{2}$. Note that this term doesn’t just penalize more parameters, it also says that if you have more data, you expect those extra parameters to help you that much more.
# 
# **Note: The BIC approximation is implemented in the GaussianMixture object of sklearn. Just call <code>model.bic(X)</code>.*

# ## 1.3. Experiment: Clustering a Wine dataset
# 
# In this experiment, we're going to fit a GMM to the [Wine Dataset](https://archive.ics.uci.edu/ml/datasets/Wine+Quality). These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.
# 

# In[6]:


wines = pd.read_csv('wines.csv')
N=len(wines)
print('N='+str(N))
wines_data = wines.values
wines.head()


# In[7]:


# Normalize the data
def normalize(data):
    mean = np.mean(data)
    var = np.var(data)
    data -= mean
    data /= var

    return data

wines_data = normalize(wines_data)


# **TASK: Make a validation using the BIC criterion to choose the number of components that best explains the dataset. Use $K=[1, 2, .., 10]$. Plot the BIC curve.**
# 
# **Which is the optimal number of clusters that explains the wines dataset?**.
# - Your answer
# 

# In[10]:


def bic_val(data, K_list):
      """
      Performs model selection using BIC to choose the optimal number of components
      for a Gaussian Mixture Model.

      Args:
          data: The dataset to fit the GMM to.
          K_list: A list of candidate values for the number of components (K).

      Returns:
          The optimal number of components (K) based on the BIC criterion.
      """

      lowest_bic = np.infty  # Initialize with a very large value
      best_K = -1  # Initialize with an invalid value

      for K in K_list:
          # Fit a GMM with the current K
          gmm = GMM(n_components=K, covariance_type='full').fit(data)

          # Calculate BIC
          bic = gmm.bic(data)

          # Update best_K if the current BIC is lower
          if bic < lowest_bic:
              lowest_bic = bic
              best_K = K

      return best_K




# **For the selected $K$, compute the number of wines associated to each cluster.**
# 
# > Ajouter une citation
# 
# 
# 

# In[11]:


# Assuming you have already determined the optimal K using bic_val
K = bic_val(wines_data, np.arange(1, 10))

# Fit a GMM with the optimal K
gmm = GMM(n_components=K, covariance_type='full').fit(wines_data)

# Predict the cluster assignments for each wine
cluster_assignments = gmm.predict(wines_data)

# Count the number of wines in each cluster
cluster_counts = np.bincount(cluster_assignments)

# Print the results
for i, count in enumerate(cluster_counts):
    print(f"Cluster {i}: {count} wines")


# ## 1.3. Experiment: MRI Image segmentation
# 
# In GMMs, we can obtain the posterior probability of each observation belonging to a specific cluster, which helps interpreting the data. One application of clustering algorithms is image segmentation, which consists of separating meaningful parts of an image. Each pixel in the image is substituted by the mean of the associated cluster (maximizing the responsibilities $r_{ik}$ wrt $k$).
# 
# For example, in MRI brain images, we can perform segmentation with GMMs by assigning the most probable cluster to each datapoint, and this will give us an estimation of different parts of the brain (Cerebrospinal Fluid, Grey Matter, White Matter). These parts are usually extracted as a initial stage for other applications, as, for example, Alzheimer detection.
# 
# We are going to fit a GMM to a sample image from [IBSR dataset](https://www.nitrc.org/projects/ibsr).
# 

# In[8]:


# Read the image
img=mpimg.imread('mri.png')
plt.gray()
plt.imshow(img)

# We want to obtain only the points inside the skull
X_all = np.reshape(img.copy(), [-1, 1])

# Set the background to black
out = X_all<0.035  # threshold to consider black
X_all[out] = 0

# Select pixels inside the skull
mask = X_all!=0 # This will filter black pixels
X = X_all[mask][:, np.newaxis]


# **TASK: Validate the number of components $K$ using the BIC criterion, and for the chosen $K$, obtain the segmented representation of the brain image. Plot the original and segmented images together to appreciate better the effect of segmentation. The components are supposed to represent, at least: the CSF, WM, GM and cortex tissues.**

# In[12]:


import matplotlib.pyplot as plt
import numpy as np
from sklearn.mixture import GaussianMixture as GMM
import matplotlib.image as mpimg

# ... (Previous code for reading and preprocessing the image) ...

# Validate the number of components using BIC
K_list = np.arange(1, 10)
best_K = bic_val(X, K_list)  # Assuming bic_val function is defined
print(f"Selected K: {best_K}")

# Fit a GMM with the optimal K
gmm = GMM(n_components=best_K, covariance_type='full').fit(X)

# Obtain the segmented representation
segm = gmm.predict(X)
segm = gmm.means_[segm]

# Add the background
segm_all = np.zeros_like(X_all)
segm_all[mask] = np.squeeze(segm)

# Reshape to original shape
segm_all = np.reshape(segm_all, img.shape)

# Plot the original and segmented images
fig, axes = plt.subplots(1, 2, figsize=(10, 5))
axes[0].imshow(img, cmap='gray')
axes[0].set_title('Original Image')
axes[1].imshow(segm_all, cmap='gray')
axes[1].set_title('Segmented Image')
plt.show()


# # 2. Mixtures of Bernoulli distributions
# 
# In this part of the notebook you will implement the EM algorithm for mixtures of Bernoulli distributions. This model is also known as [*latent class analysis*](https://en.wikipedia.org/wiki/Latent_class_model). As well as being of practical importance on its own right, understading this model and how it learns also lays the fundations for **hidden Markov models (HMMs)** over discrete variables. HMMs will be dicussed in future course sessions.
# 
# Consider a set of i.i.d. $D$-dimensional binary (0-1) vectors. Examples of this kind of data are binary images, binary detection results, or genetic markers. Consider also a mixture of multinomials (or multivariate Bernoullis) model for each of the vectors, $\mathbf{x}$,
# 
# $$\displaystyle p(\mathbf{x} | \boldsymbol{\Theta},\mathbf{\pi} ) = \sum_{k=1}^K \pi_k p_k(\mathbf{x}^{(i)}|\boldsymbol{\theta}_k) = \sum_{k=1}^K \pi_k \prod_{j=1}^D \theta_{jk}^{x_{j}} (1-\theta_{jk})^{1-x_{j}},$$
# where $\boldsymbol{\Theta}=[\boldsymbol{\theta}_1,\ldots,\boldsymbol{\theta}_K]$. If we are given a data set $\mathbf{X}=\{\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)}\}$ then the log likelihood function for this model is given by
# 
# \begin{align}
# \log p(\mathbf{X}|\boldsymbol{\Theta},\mathbf{\pi}) = \sum_{i=1}^N \log\left(\sum_{k=1}^D \pi_k p(\mathbf{x}^{(i)}|\boldsymbol{\theta}_k)\right)
# \end{align}
# 
# Again, we have the $\log$ of a sum, which remains non-convex and is hard to optimize.
# 
# In contrast to the GMM, in this model we add an extra feature to prevent overfitting: we will use a **prior distribution** for the model parameters. For $\boldsymbol{\Theta}$ we have
# 
# \begin{align}
# \theta_{jk}&\sim\text{Beta}(\alpha,\beta)\\
# p(\boldsymbol{\Theta}) &= \prod_{k=1}^{K} \prod_{j=1}^D p(\theta_{jk})\\
# p(\theta_{jk}) &= \frac{\theta_{jk}^{\alpha-1}(1-\theta_{jk})^{\beta-1}}{\text{B}(\alpha,\beta)},
# \end{align}
# where $\text{B}(\alpha,\beta)$ is the [Beta function](https://en.wikipedia.org/wiki/Beta_function), that we have studied in previous sessions.
# 
# For $\mathbf{\pi}$ we use a uniform [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution) (that we have also studied):
# \begin{align}
# \mathbf{\pi}&\sim \text{Dir}(\frac{1}{K}, \ldots, \frac{1}{K}) \Rightarrow p(\mathbf{\pi}) =\frac{1}{\text{B}(\frac{1}{K}, \ldots, \frac{1}{K})}\prod_{k=1}^{K}\pi_k^{\frac{1}{K}-1},
# \end{align}
# where $\text{B}(\frac{1}{K}, \ldots, \frac{1}{K})$ is the multivariate Beta function.
# 

# We now derive the EM algorithm for maximizing the **posterior** distribution $p(\mathbf{\Theta},\mathbf{\pi}|\mathbf{X})$. To do this, we introduce an explict discrete latent variable $z\in\{1,\ldots,K\}$ associated to each data point $\mathbf{x}$:
# \begin{align}
# p(\mathbf{x},z) = \prod_{k=1}^{K} \left(\pi_k ~p_{k}(\mathbf{x})\right)^{\mathbb{1} [z==k]}, ~~ p(z)=\prod_{k=1}^K \pi_k^{\mathbb{1} [z==k]}
# \end{align}
# 
# #### Complete log-likelihood
# 
# We write the complete data log-likelihood as follows:
# \begin{align}
# \log p(\mathbf{X},\mathbf{z}|\boldsymbol{\Theta},\mathbf{\pi})=\sum_{i=1}^{N}\sum_{k=1}^{K}\mathbb{1} [z^{(i)}==k]\left(\log \pi_k + \sum_{j=1}^D \left[x_{j}^{(i)}\log\theta_k+(1-x_{j}^{(i)})\log(1-\theta_k)\right]\right)
# \end{align}
# 
# #### Posterior distribution of $\mathbf{z}$ given $\boldsymbol{\Theta},\mathbf{\pi}$
# 
# In the $E$-step, we have to compute the expected complete data log-likelihood w.r.t. the posterior distribution of $\mathbf{z}$ given the current values of $\boldsymbol{\Theta},\mathbf{\pi}$:
# \begin{align}
# p(z^{(i)}=k|\mathbf{x}^{(i)},\boldsymbol{\Theta}_{(t-1)},\mathbf{\pi}_{(t-1)}) \triangleq r_{ik} = \frac{\pi_{(k,t-1)} p_k(\mathbf{x}|\boldsymbol{\theta}_k) }{\sum_{q=1}^K \pi_{(q,t-1)} p_k(\mathbf{x}^{(i)}|\boldsymbol{\theta}_q)}, ~~~ k=1,\ldots, K
# \end{align}

# ## 2.1. The EM algorithm for a mixture of Bernoullis
# 
# #### E-step
# It is easy to show that
# \begin{align}
# \mathcal{Q}(\boldsymbol{\Theta},\mathbf{\pi},\boldsymbol{\Theta}_{(t-1)},\mathbf{\pi}_{(t-1)})&=\mathbb{E}_{p(\mathbf{z}|\mathbf{X}^{(i)},\boldsymbol{\Theta}_{(t-1)},\mathbf{\pi}_{(t-1)})}[\log p(\mathbf{X},\mathbf{z}|\boldsymbol{\Theta},\mathbf{\pi})]\\
# &=\sum_{i=1}^{N}\sum_{k=1}^K r_{ik} \left(\log \pi_k + \sum_{j=1}^D \left[x_{j}^{(i)}\log\theta_k+(1-x_{j}^{(i)})\log(1-\theta_k)\right]\right)
# \end{align}
# 
# #### M-step
# 
# We have to find
# \begin{align}
# \boldsymbol{\Theta}_t,\mathbf{\pi}_t = \arg \max_{\boldsymbol{\Theta},\mathbf{\pi}} ~~\mathcal{Q}(\boldsymbol{\Theta},\mathbf{\pi},\boldsymbol{\Theta}_{(t-1)},\mathbf{\pi}_{(t-1)})+ \log p(\boldsymbol{\Theta})+\log p(\mathbf{\pi})
# \end{align}
# 
# As a result, one can prove that the maximum is attained at
# 
# \begin{align}
# r_k &\triangleq \sum_{i=1}^N r_{ik} \\\\
# \pi^t_k &= \frac{r_k+\frac{1}{K}-1}{N+1-K}\\\\
# \boldsymbol{\theta}_k &= \frac{\sum_{i=1}^N r_{ik}\mathbf{x}^{(i)}+\alpha-1}{r_k+\alpha+\beta-2}
# \end{align}
# 
# To see details about the derivation of this result, check out chapter 9 of Bishop's book. Also Chapter 11 of Murphy's book.

# ## 2.2. Experiment: Implementing the EM algorithm for a Mixture of Bernoullis
# 
# **TASK: Given the functions with all the steps implemented, build a higher-level function that performs the EM algorithm. The E-Step should compute $r_{ik}$, with $i=1,\ldots,N$, $k=1,\ldots,K$ for each data point. It should take the current values of $\boldsymbol{\pi}$, $\boldsymbol{\Theta}$ as input, and the matrix $\mathbf{X}$ of observations. The M-Step should update the values of $\boldsymbol{\pi}$, $\boldsymbol{\Theta}$.**

# In[21]:


def bernoulli_pdf(X,Thetak):
    M = np.exp(X*np.log(Thetak+1e-2)+(1-X)*np.log(1-Thetak+1e-2))
    P = np.prod(M,1)
    return P

def responsibilities(X,P,Theta,K):
    N,D = X.shape
    R = np.zeros([N,K])

    for k in range(K):
        R[:,k] = bernoulli_pdf(X,Theta[k,:])

    R *= P.T
    R /= np.sum(R,1).reshape([-1,1])

    return R


# In[17]:


def points_cluster(R):
    return np.sum(R,0)

def new_Pi(R,N):
    Rk = points_cluster(R)
    return Rk/N

def new_Theta(R,X,K,alpha,beta):

    Theta = np.zeros([K,D])
    Rk = points_cluster(R)

    for k in range(K):
        Theta[k,:] = (np.sum(X*R[:,k].reshape([-1,1]),0)+alpha-1) / (Rk[k]+alpha+beta-2)
    return Theta

def log_lik(X,P,Theta,K):
    LL = 0.
    for k in range(K):
        LL += bernoulli_pdf(X,Theta[k,:])*P[k]
    return np.sum(np.log(LL)) #Your code here


# In[16]:


import numpy as np

def EM_bernouilli(X, K, Niter):
    """
    Performs the Expectation-Maximization (EM) algorithm for a Mixture of Bernoullis.

    Args:
        X: The data matrix (N x D), where N is the number of samples and D is the dimensionality.
        K: The number of clusters.
        Niter: The maximum number of iterations.

    Returns:
        Pi: The mixing weights (K x 1).
        Theta: The cluster parameters (K x D).
        R: The responsibilities (N x K).
        LL: The log-likelihood.
    """

    # Define the prior of Theta with alpha and beta
    alpha = 2.0
    beta = 2.0

    N, D = X.shape  # Get data dimensions

    # Initialize Pi and Theta randomly
    Pi = np.random.rand(K)
    Pi /= np.sum(Pi)  # Normalize Pi to sum to 1
    Theta = np.random.rand(K, D)

    # EM iterations
    LL = []  # Store log-likelihood values
    for _ in range(Niter):
        # E-Step: Calculate responsibilities
        R = responsibilities(X, Pi, Theta, K)

        # Compute log-likelihood before M-Step (for monitoring convergence)
        LL.append(log_lik(X, Pi, Theta, K))

        # M-Step: Update Pi and Theta
        Pi = new_Pi(R, N)
        Theta = new_Theta(R, X, K, alpha, beta)

    return Pi, Theta, R, LL


# ## 2.3. EM for a Digit Dataset
# 
# In this section you will train a Mixture of Bernoullis to a binarized version of [this digit dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html).

# In[18]:


from sklearn.datasets import load_digits

# Load digits dataset
digits = load_digits()    #Gray scale, we have to binarize

D = digits.data.shape[-1]
N = digits.data.shape[0]

f, ax = plt.subplots(1, 2, figsize=(4, 2))
plt.gray()
ax[0].imshow(digits.images[15])
ax[0].set_title('Original')

#Binarization
Bin_Images = np.copy(digits.images)
val_min=np.min(Bin_Images)
val_max=np.max(Bin_Images)
Bin_Images = (Bin_Images - val_min) / (val_max - val_min)
Bin_Images = np.round(Bin_Images)

ax[1].imshow(Bin_Images[15])
ax[1].set_title('Binarized')

# Reshape to obtain vector observations
X = Bin_Images.reshape([-1,D])


# **TASK: Train a Mixture of Bernoullis for the digits dataset. Choose the number of clusters $K$ by common sense, without necessarily validating.**

# In[22]:


# ... (Previous code for loading and preprocessing the digits dataset) ...

# Set the number of clusters (K)
K = 10  # Common sense choice, as there are 10 digits (0-9)

# Train the Mixture of Bernoullis using the EM algorithm
Pi, Theta, R, LL = EM_bernouilli(X, K, Niter=100)  # Adjust Niter as needed


# [texte du lien](https://)**TASK: Plot the negative log-likelihood obtained at each iteration using a logarithmic y-axis (the log-likelihood should decrease). *Note: Compute it before the E-Step.***

# In[23]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits

# ... (functions: responsibilities, log_lik, new_Pi, new_Theta, EM_bernouilli remain the same) ...

# Load digits dataset
# ... (Code for loading and preprocessing the digits dataset remains the same) ...

# Set the number of clusters (K)
K = 10  # Common sense choice, as there are 10 digits (0-9)

# Train the Mixture of Bernoullis using the EM algorithm
Niter = 100  # Ensure Niter is an integer
Pi, Theta, R, LL = EM_bernouilli(X, K, Niter)

# Plot the negative log-likelihood
plt.figure(figsize=(8, 6))
plt.plot(range(1, Niter + 1), [-ll for ll in LL])  # Plot negative log-likelihood
plt.yscale('log')  # Set y-axis to logarithmic scale
plt.xlabel('Iteration')
plt.ylabel('Negative Log-Likelihood')
plt.title('Convergence of EM Algorithm')
plt.grid(True)
plt.show()


# **TASK: Plot each $\theta_k$ vector as a 8x8 gray-scale image (reshaping from 64 to (8, 8)). It represents the probability for each pixel to be 1, for each cluster $k$.**

# In[24]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits

# ... (functions: responsibilities, log_lik, new_Pi, new_Theta, EM_bernouilli remain the same) ...

# Load digits dataset
# ... (Code for loading and preprocessing the digits dataset remains the same) ...

# Set the number of clusters (K)
K = 10  # Common sense choice, as there are 10 digits (0-9)

# Train the Mixture of Bernoullis using the EM algorithm
Niter = 100  # Ensure Niter is an integer
Pi, Theta, R, LL = EM_bernouilli(X, K, Niter)

# ... (Code for plotting the negative log-likelihood remains the same) ...

# Visualize the cluster parameters (Theta) as images
plt.figure(figsize=(10, 5))
for k in range(K):
    plt.subplot(2, 5, k + 1)  # Create subplots for each cluster
    plt.imshow(Theta[k, :].reshape(8, 8), cmap='gray')  # Reshape Theta to 8x8 image
    plt.title(f'Cluster {k}')
    plt.axis('off')  # Turn off axes

plt.suptitle('Visualizing Cluster Parameters (Theta)')
plt.tight_layout()
plt.show()


# ## References
# 
# [1]. Murphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT press.
# 
# [2]. Bishop, C. M. (2006). Pattern recognition and machine learning. Springer.
# 
# 
